{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-07 00:33:59.356248: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-07 00:33:59.366390: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-07 00:33:59.366409: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-07 00:33:59.373391: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-07 00:33:59.770060: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/james/projects/pythonProjects/TF7_final_project/model_pre_train\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.api.layers import *\n",
    "from keras.api.models import Model, Sequential\n",
    "from keras.api.regularizers import l2\n",
    "from keras.api.optimizers import Adam\n",
    "import keras.api.backend as K\n",
    "import keras\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "\n",
    "from module_design import model_structure\n",
    "\n",
    "root_path = os.getcwd()\n",
    "print(root_path)\n",
    "\n",
    "board_size = 15\n",
    "last_n_feature=1\n",
    "l2_const = 1e-4\n",
    "batch_size=512\n",
    "epochs=50\n",
    "init_lr=1e-4\n",
    "is_load_model=False\n",
    "load_model_dir=None# xx_xx_xxxxxx\n",
    "\n",
    "sample_size=1900 # number of games choose from data\n",
    "take_last_n_move=10 # number of last steps used in each game\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading file trans_from_playok...\n",
      "file /home/james/projects/pythonProjects/TF7_final_project/model_pre_train/../data_transform/tmp/training_data_v1/trans_from_playok loaded\n",
      "total of 1925 game(s) loaded\n",
      "total of 1900 game(s) used\n",
      "total status:9500\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join(root_path,f'../data_transform/tmp/training_data_v{last_n_feature}')\n",
    "data_files = os.listdir(data_dir)\n",
    "data = []\n",
    "for file in data_files:\n",
    "    print(f\"loading file {file}...\")\n",
    "    file = os.path.join(data_dir,file)\n",
    "    with open(file, 'rb') as f:\n",
    "        data += pickle.load(f)\n",
    "    print(f\"file {file} loaded\")\n",
    "print(f\"total of {len(data)} game(s) loaded\")\n",
    "\n",
    "status = []\n",
    "probs = []\n",
    "wins = []\n",
    "result = []\n",
    "moves = []\n",
    "if sample_size > len(data): \n",
    "    sample_size=len(data)\n",
    "data=random.sample(data,sample_size)\n",
    "print(f\"total of {len(data)} game(s) used\")\n",
    "\n",
    "for game in data:\n",
    "    for move_n in game:\n",
    "        moves.append(move_n)\n",
    "random.shuffle(moves)\n",
    "# print(moves[0])\n",
    "for i in moves:\n",
    "    status.append(i[0])\n",
    "    probs.append(i[1])\n",
    "    wins.append(i[2])\n",
    "\n",
    "train_size = int(0.8*len(status))\n",
    "print(\"total status:{}\".format(len(status)))\n",
    "\n",
    "x_train = status[:train_size]\n",
    "x_test = status[train_size:]\n",
    "\n",
    "y_train_probs = probs[:train_size]\n",
    "y_test_probs = probs[train_size:]\n",
    "\n",
    "y_train_wins = wins[:train_size]\n",
    "y_test_wins = wins[train_size:]\n",
    "\n",
    "x_train = np.asarray(x_train)\n",
    "x_test = np.asarray(x_test)\n",
    "\n",
    "y_train_probs = np.asarray(y_train_probs)\n",
    "y_train_probs = np.reshape(y_train_probs, (y_train_probs.shape[0], -1))\n",
    "y_train_wins = np.asarray(y_train_wins)\n",
    "\n",
    "y_test_probs = np.asarray(y_test_probs)\n",
    "y_test_probs = np.reshape(y_test_probs, (y_test_probs.shape[0], -1))\n",
    "y_test_wins = np.asarray(y_test_wins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1720283768.981987   20301 service.cc:145] XLA service 0x7fb800003da0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1720283768.982006   20301 service.cc:153]   StreamExecutor device (0): NVIDIA Graphics Device, Compute Capability 8.9\n",
      "2024-07-07 00:36:10.459944: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-07-07 00:36:15.615135: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1720283780.753581   20869 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_58320', 412 bytes spill stores, 412 bytes spill loads\n",
      "\n",
      "I0000 00:00:1720283780.819296   20881 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_58563', 244 bytes spill stores, 244 bytes spill loads\n",
      "\n",
      "I0000 00:00:1720283781.121812   20871 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_58563', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "I0000 00:00:1720283781.222979   20878 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_58127', 2344 bytes spill stores, 2340 bytes spill loads\n",
      "\n",
      "I0000 00:00:1720283781.251637   20882 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_58127', 212 bytes spill stores, 212 bytes spill loads\n",
      "\n",
      "I0000 00:00:1720283781.263091   20884 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_58320', 60 bytes spill stores, 60 bytes spill loads\n",
      "\n",
      "I0000 00:00:1720283781.406442   20874 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_58570', 40 bytes spill stores, 40 bytes spill loads\n",
      "\n",
      "I0000 00:00:1720283781.515771   20883 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_58127', 360 bytes spill stores, 360 bytes spill loads\n",
      "\n",
      "I0000 00:00:1720283781.560915   20888 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_58910', 168 bytes spill stores, 144 bytes spill loads\n",
      "\n",
      "I0000 00:00:1720283781.560958   20885 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_58127', 328 bytes spill stores, 324 bytes spill loads\n",
      "\n",
      "I0000 00:00:1720283781.855340   20884 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_58320', 412 bytes spill stores, 412 bytes spill loads\n",
      "\n",
      "I0000 00:00:1720283781.855481   20865 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_58908', 92 bytes spill stores, 92 bytes spill loads\n",
      "\n",
      "I0000 00:00:1720283781.918432   20868 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_58127', 196 bytes spill stores, 196 bytes spill loads\n",
      "\n",
      "I0000 00:00:1720283782.025921   20877 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_58910', 96 bytes spill stores, 96 bytes spill loads\n",
      "\n",
      "I0000 00:00:1720283782.330658   20869 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_58908', 148 bytes spill stores, 116 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1720283824.040388   20301 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(e)\n\u001b[0;32m---> 37\u001b[0m result\u001b[38;5;241m=\u001b[39m\u001b[43mhistory\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "if(is_load_model):\n",
    "    model=load_model(os.path.join(root_path,'model_record',load_model_dir,'model.h5'))\n",
    "else:\n",
    "    model=model_structure(board_size,last_n_feature,l2_const)\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=init_lr,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)\n",
    "\n",
    "optimizer = Adam(\n",
    "    learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss={'policy_net': 'categorical_crossentropy', 'value_net': 'mean_squared_error'},\n",
    "    metrics={\n",
    "        'policy_net': ['accuracy'], \n",
    "        'value_net': ['mae', 'mean_squared_error']\n",
    "    })\n",
    "\n",
    "# print(model.summary())\n",
    "\n",
    "try:\n",
    "    start_time=time.process_time()\n",
    "    history = model.fit(x_train,\n",
    "                    [y_train_probs, y_train_wins],\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.2,\n",
    "                    validation_data=(x_test, [y_test_probs, y_test_wins]))\n",
    "    process_time=time.process_time()-start_time\n",
    "except KeyboardInterrupt as e:\n",
    "    print(e)\n",
    "\n",
    "result=history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.history.keys())\n",
    "\n",
    "policy_net_accuracy = history.history['policy_net_accuracy']\n",
    "val_policy_net_accuracy = history.history['val_policy_net_accuracy']\n",
    "value_net_mae = history.history['value_net_mae']\n",
    "val_value_net_mae = history.history['val_value_net_mae']\n",
    "value_net_mse = history.history['value_net_mean_squared_error']\n",
    "val_value_net_mse = history.history['val_value_net_mean_squared_error']\n",
    "\n",
    "# Plotting Policy Net Accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(policy_net_accuracy, label='Policy Net Training Accuracy')\n",
    "plt.plot(val_policy_net_accuracy, label='Policy Net Validation Accuracy')\n",
    "plt.title('Policy Net Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting Value Net MAE\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(value_net_mae, label='Value Net Training MAE')\n",
    "plt.plot(val_value_net_mae, label='Value Net Validation MAE')\n",
    "plt.title('Value Net Training and Validation MAE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting Value Net MSE\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(value_net_mse, label='Value Net Training MSE')\n",
    "plt.plot(val_value_net_mse, label='Value Net Validation MSE')\n",
    "plt.title('Value Net Training and Validation MSE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "t = time.localtime()\n",
    "folder_name = f\"{t.tm_mon:0>2}_{t.tm_mday:0>2}_{t.tm_hour:0>2}{t.tm_min:0>2}{t.tm_sec:0>2}\"\n",
    "new_folder_dir=os.path.join(root_path,'model_pre_train/model_record',folder_name)\n",
    "os.makedirs(new_folder_dir, exist_ok=True)\n",
    "\n",
    "plt.savefig(os.path.join(new_folder_dir,'history.png'))\n",
    "plt.show()\n",
    "\n",
    "keras.saving.save_model(model, os.path.join(new_folder_dir,'model.keras'))\n",
    "\n",
    "def convert_seconds(seconds):\n",
    "    seconds=int(seconds)\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    seconds = seconds % 60\n",
    "    return f\"{hours:0>3}h {minutes:0>2}m {seconds:0>2}s\"\n",
    "\n",
    "score = model.evaluate(\n",
    "    x_test, [y_test_probs, y_test_wins], verbose=1)\n",
    "\n",
    "with open(os.path.join(new_folder_dir,'result.txt'), 'w') as f:\n",
    "    f.write(f\"board size = {board_size}\\n\")\n",
    "    f.write(f\"load model = {is_load_model}\\n\")\n",
    "    f.write(f\"load model dir = {load_model_dir}\\n\")\n",
    "    f.write(f\"epochs = {epochs}\\n\")\n",
    "    f.write(f\"batch_size = {batch_size}\\n\")\n",
    "    f.write(f\"sample_size = {sample_size}\\n\")\n",
    "    f.write(f\"take_last_n_move = {take_last_n_move}\\n\")\n",
    "    f.write(f\"last_n_feature = {last_n_feature}\\n\")\n",
    "    f.write(f\"init lr = {init_lr}\\n\")\n",
    "    f.write(f\"l2 const = {l2_const}\\n\")\n",
    "    f.write(f\"loss = {score[0]}\\n\")\n",
    "    f.write(f\"policy_net_accuracy = {score[1]}\\n\")\n",
    "    f.write(f\"value_net_mae = {score[2]}\\n\")\n",
    "    f.write(f\"value_net_mean_squared_error = {score[3]}\\n\")\n",
    "    f.write(f\"process time = {convert_seconds(process_time)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
